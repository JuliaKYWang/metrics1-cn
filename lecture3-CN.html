
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. 最小二乘法：线性代数观点 &#8212; 计量经济学</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="2. 基本渐近理论" href="lecture5-CN.html" />
    <link rel="prev" title="介绍" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">计量经济学</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   介绍
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   1. 最小二乘法：线性代数观点
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lecture5-CN.html">
   2. 基本渐近理论
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/lecture3-CN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flecture3-CN.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/lecture3-CN.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimator">
   1.1. 估计量 (Estimator)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subvector">
   1.2. 子向量 (Subvector)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#goodness-of-fit">
   1.3. 适配度检验 (Goodness of Fit)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   1.4. 总结
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>最小二乘法：线性代数观点</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimator">
   1.1. 估计量 (Estimator)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#subvector">
   1.2. 子向量 (Subvector)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#goodness-of-fit">
   1.3. 适配度检验 (Goodness of Fit)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   1.4. 总结
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1><span class="section-number">1. </span>最小二乘法：线性代数观点<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="admonition- admonition">
<p class="admonition-title">《九章算术》第八章：方程</p>
<p>今有卖牛二、羊五, 以买十三豕, 有馀钱一千. 卖牛三、豕三, 以买九羊, 钱适足. 卖羊六、豕八, 以买五牛, 钱不足六百. 问牛、羊、豕价各几何?</p>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\left[\begin{array}{r} 
2 &amp; 5 &amp; -13\\
3 &amp; -9 &amp; 3\\
-5 &amp; 6 &amp; 8
\end{array} \right]
\left[\begin{array}{c}
牛\\
羊\\
猪
\end{array} \right]
=
\left[\begin{array}{r}
1000\\
0\\
-600
\end{array} \right]
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">(</span><span class="n">c</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">13</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">nrow</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">c</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">600</span><span class="p">)</span>
<span class="n">solve</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<p>最小二乘法 (OLS) 是计量经济学中最基本的估计模型, 它简单透明, 易于理解. 了解最小二乘法, 有助于我们研究更复杂的线性估计方法. 此外, 许多非线性估计量在真实值附近与线性估计量的行为是类似的. 在本讲义中, 我们将从线性代数的运算讲起, 学习一系列的知识.</p>
<p>套用Leopold Kronecker的名言 “上帝创造了整数, 其他都是人的作品”, 我想说 “高斯创造了最小二乘法, 其他都是应用研究者的作品”. 在科学界, 最小二乘法的受欢迎程度远远超出了人们的想象. 但要注意的是, 最小二乘法是一种纯统计学的方法, 或者说是一种监督式机器学习的方法, 因此它揭示的是相关关系, 而非因果关系. 相反, 经济理论假设因果关系的存在, 然后我们收集数据来检验理论或量化效果.</p>
<p>数学标记: <span class="math notranslate nohighlight">\(y_{i}\)</span> 是标量. <span class="math notranslate nohighlight">\(x_{i}=\left(x_{i1},\ldots,x_{iK}\right)'\)</span> 是 <span class="math notranslate nohighlight">\(K\times1\)</span> 的向量. <span class="math notranslate nohighlight">\(Y=\left(y_{1},\ldots,y_{n}\right)'\)</span> 是 <span class="math notranslate nohighlight">\( n\times1\)</span> 的向量.</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
X=\left[\begin{array}{c}
x_{1}'\\
x_{2}'\\
\vdots\\
x_{n}'
\end{array}\right]=\left[\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1K}\\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2K}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n1} &amp; x_{22} &amp; \cdots &amp; x_{nK}
\end{array}\right]
\end{split}\]</div>
<p>是 <span class="math notranslate nohighlight">\(n\times K\)</span> 的矩阵. <span class="math notranslate nohighlight">\(I_{n}\)</span> 是 <span class="math notranslate nohighlight">\(n\times n\)</span> 的单位矩阵.</p>
<div class="section" id="estimator">
<h2><span class="section-number">1.1. </span>估计量 (Estimator)<a class="headerlink" href="#estimator" title="Permalink to this headline">¶</a></h2>
<p>基于我们已经在前一讲中学习过的线性投影模型</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}y &amp; =x'\beta+e\end{aligned}, 
\]</div>
<p>投影系数 <span class="math notranslate nohighlight">\(\beta\)</span> 可以写作</p>
<div class="math notranslate nohighlight" id="equation-eqn-1">
<span class="eqno">(1.1)<a class="headerlink" href="#equation-eqn-1" title="Permalink to this equation">¶</a></span>\[
\beta=\left(E\left[xx'\right]\right)^{-1}E\left[xy\right]. 
\]</div>
<p>我们从 <span class="math notranslate nohighlight">\(\left(y,x\right)\)</span> 的联合分布中取出一个观测值, 记作 <span class="math notranslate nohighlight">\(\left(y_{i},x_{i}\right)\)</span>. 重复此过程 <span class="math notranslate nohighlight">\(n\)</span> 次得到 <span class="math notranslate nohighlight">\(n\)</span> 个观测值, 即 <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>, 那么我们随即就得到了一个样本 <span class="math notranslate nohighlight">\(\left(y_{i},x_{i}\right)_{i=1}^{n}\)</span>.</p>
<div class="proof remark admonition" id="remark31">
<p class="admonition-title"><span class="caption-number">Remark 1.1 </span></p>
<div class="remark-content section" id="proof-content">
<p>样本 <span class="math notranslate nohighlight">\(\left(y_{i},x_{i}\right)\)</span> 到底是随机的呢？还是固定的呢？</p>
<p>——在我们观测之前, 样本是随机变量, 而随机变量的值是不确定的. 当我们谈起样本的统计学性质时, 我们必须将其视之为随机变量, 因为只有随机变量才有统计学性质, 固定值的统计学性质是无意义的. 而在我们观测之后, 样本的值就确定下来了, 成为固定值, 不能再更改.</p>
</div>
</div><div class="proof remark admonition" id="remark32">
<p class="admonition-title"><span class="caption-number">Remark 1.2 </span></p>
<div class="remark-content section" id="proof-content">
<p>在实际操作中, 我们手中只有一些给定的数据 (当然, 现在的大数据也可以将文本, 照片声音和图像处理成为数据, 这些数据在计算机当中用0和1来表示) . 我们把这些数据扔给计算机, 让计算机给出一个结果. 在统计学意义上, 我们认为这些数字是从一个概率分布上得出的<strong>思想实验</strong>结果. 思想实验是一个学术用语, 说白了, 它就是一个故事. 在公理体系统治的概率论当中, 这个故事在数学上是自洽的. 但是要知道，数学本身是一个套套逻辑 (tautology) , 而不是科学.</p>
<p>而概率模型的科学价值恰恰在于, 它到底在多大程度上能够逼近事实的真相？以及, 它是不是能够帮我们预测一些真相？在这门课中, 我们假设数据来自于某种机制, 我们把这种机制当成真相. 比如, 在线性回归当中, <span class="math notranslate nohighlight">\(\left(y,x\right)\)</span> 的联合分布就是我们头脑中的真相. 而我们想要研究的线性投影系数 <span class="math notranslate nohighlight">\(\beta\)</span> 即为此真相的某个侧面表现形式.</p>
</div>
</div><p>样本均值 (sample mean) 是总体均值 (population mean) 的天然估计量. 将式 <a class="reference internal" href="#equation-eqn-1">(1.1)</a> 中总体均值 <span class="math notranslate nohighlight">\(E\left[\cdot\right]\)</span> 替换为样本均值 <span class="math notranslate nohighlight">\(\frac{1}{n}\sum_{i=1}^{n}\cdot\)</span>, 那么相应的, 最小二乘法系数的估计值就可以写作</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{\beta} &amp; =\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}'\right)^{-1}\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}\\
 &amp; =\left(\frac{X'X}{n}\right)^{-1}\frac{X'y}{n}=\left(X'X\right)^{-1}X'y.\end{aligned}
\end{split}\]</div>
<p>上式假设 <span class="math notranslate nohighlight">\(X'X\)</span> 是可逆的. 这是一种理解最小二乘法的观点.</p>
<p>我们也可以从最小化残差平方和 <span class="math notranslate nohighlight">\(\sum_{i=1}^{n}\left(y_{i}-x_{i}'b\right)^{2}\)</span> 的角度来得到最小二乘法估计量, 这等价于最小化</p>
<div class="math notranslate nohighlight">
\[
Q\left(b\right)=\frac{1}{2n}\sum_{i=1}^{n}\left(y_{i}-x_{i}'b\right)^{2}=\frac{1}{2n}\left(Y-Xb\right)'\left(Y-Xb\right)=\frac{1}{2n}\left\Vert Y-Xb\right\Vert ^{2}.
\]</div>
<p>其中, 系数 <span class="math notranslate nohighlight">\(\frac{1}{2n}\)</span> 与 <span class="math notranslate nohighlight">\(b\)</span> 无关, 不会影响该最小化问题的解. <span class="math notranslate nohighlight">\(\left\Vert \cdot\right\Vert\)</span> 是向量的欧几里得范数(Euclidean norm).</p>
<p><span class="math notranslate nohighlight">\(Q\left(b\right)\)</span> 最小化的一阶条件是</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial}{\partial b}Q\left(b\right)=\left[\begin{array}{c}
\partial Q\left(b\right)/\partial b_{1}\\
\partial Q\left(b\right)/\partial b_{2}\\
\vdots\\
\partial Q\left(b\right)/\partial b_{K}
\end{array}\right]=-\frac{1}{n}X'\left(Y-Xb\right)=0.
\end{split}\]</div>
<p>因此, 最小化 <span class="math notranslate nohighlight">\(Q\left(b\right)\)</span> 的必要条件让我们得到了相同的估计值 <span class="math notranslate nohighlight">\(b=\widehat{\beta}=\left(X'X\right)^{-1}X'y\)</span>.</p>
<p>另外, 二阶条件</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial^{2}}{\partial b\partial b'}Q\left(b\right)=\left[\begin{array}{cccc}
\frac{\partial^{2}}{\partial b_{1}^{2}}Q\left(b\right) &amp; \frac{\partial^{2}}{\partial b_{2}\partial b_{2}}Q\left(b\right) &amp; \cdots &amp; \frac{\partial^{2}}{\partial b_{K}\partial b_{1}}Q\left(b\right)\\
\frac{\partial^{2}}{\partial b_{1}\partial b_{2}}Q\left(b\right) &amp; \frac{\partial^{2}}{\partial b_{2}^{2}}Q\left(b\right) &amp; \cdots &amp; \frac{\partial^{2}}{\partial b_{K}\partial b_{2}}Q\left(b\right)\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
\frac{\partial^{2}}{\partial b_{1}\partial b_{K}}Q\left(b\right) &amp; \frac{\partial^{2}}{\partial b_{2}\partial b_{K}}Q\left(b\right) &amp; \cdots &amp; \frac{\partial^{2}}{\partial b_{K}^{2}}Q\left(b\right)
\end{array}\right]=\frac{1}{n}X'X
\end{split}\]</div>
<p>表明 <span class="math notranslate nohighlight">\(Q\left(b\right)\)</span> 是关于 <span class="math notranslate nohighlight">\(b\)</span> 的凸函数, 因为 <span class="math notranslate nohighlight">\(X'X/n\)</span> 是半正定矩阵. (如果 <span class="math notranslate nohighlight">\(X'X/n\)</span> 是正定矩阵, 那么 <span class="math notranslate nohighlight">\(Q\left(b\right)\)</span> 是关于 <span class="math notranslate nohighlight">\(b\)</span> 的严格凸函数. )</p>
<div class="proof remark admonition" id="remark3">
<p class="admonition-title"><span class="caption-number">Remark 1.3 </span></p>
<div class="remark-content section" id="proof-content">
<p>在最小二乘法的推导中, 我们假定 <span class="math notranslate nohighlight">\(X\)</span> 中的 <span class="math notranslate nohighlight">\(K\)</span> 列是<em>线性独立的</em>, 也就是说不存在 <span class="math notranslate nohighlight">\(K\times1\)</span> 的向量
<span class="math notranslate nohighlight">\(b\ (b\neq0_{K})\)</span> 使得 <span class="math notranslate nohighlight">\(Xb=0_{n}\)</span>. 同时, 这也意味着 <span class="math notranslate nohighlight">\(n\geq K\)</span> , 并且 <span class="math notranslate nohighlight">\(X'X/n\)</span> 是可逆矩阵.</p>
<p>然而线性独立的假设并不永远正确, 如果某些回归因子满足<em>完全共线性 (perfectly collinear)</em> , 就违反了线性独立的假设.</p>
<p>例如, 使用虚拟变量来表示分类变量并将所有这些类别放入回归模型中时, 通常计量经济学软件会自动检测并指出完全共线性问题. 然而, 难以察觉的是<em>不完全共线性 (nearly collinear)</em>, 即 <span class="math notranslate nohighlight">\(X'X/n\)</span> 的最小特征值接近于0, 而不等于0. 我们将在渐进理论一章中讨论不完全共线性的后果.</p>
</div>
</div><div class="proof property admonition" id="property1">
<p class="admonition-title"><span class="caption-number">Property 1.1 </span></p>
<div class="property-content section" id="proof-content">
<p>下面列举一些与最小二乘法估计量有关的定义与性质.</p>
<ul class="simple">
<li><p>拟合值 (Fitted value): <span class="math notranslate nohighlight">\(\widehat{Y}=X\widehat{\beta}\)</span>.</p></li>
<li><p>投影矩阵 (Projection matrix): <span class="math notranslate nohighlight">\(P_{X}=X\left(X'X\right)^{-1}X\)</span>; 残差生成矩阵 (Residual maker
matrix) : <span class="math notranslate nohighlight">\(M_{X}=I_{n}-P_{X}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(P_{X}X=X\)</span>; <span class="math notranslate nohighlight">\(X'P_{X}=X'\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(M_{X}X=0_{n\times K}\)</span>; <span class="math notranslate nohighlight">\(X'M_{X}=0_{K\times n}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(P_{X}M_{X}=M_{X}P_{X}=0_{n\times n}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(P_{X}\)</span> 与 <span class="math notranslate nohighlight">\(M_{X}\)</span> 都是<em>幂等矩阵</em>.</p></li>
</ul>
<ul class="simple">
<li><p>如果 <span class="math notranslate nohighlight">\(AA=A\)</span>, 那么 <span class="math notranslate nohighlight">\(A\)</span> 被称作<em>幂等矩阵</em> (<em>idempotent</em> matrix). 幂等矩阵的特征值只能是1或者0.</p></li>
</ul>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathrm{rank}\left(P_{X}\right)=K\)</span>,
<span class="math notranslate nohighlight">\(\mathrm{rank}\left(M_{X}\right)=n-K\)</span>.</p></li>
<li><p>残差:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{e}&amp;=Y-\widehat{Y}=Y-X\widehat{\beta}=Y-X(X'X)^{-1}X'Y=(I_{n}-P_{X})Y=M_{X}Y\\
&amp;=M_{X}\left(X\beta+e\right)=M_{X}e.
\end{aligned}
\end{split}\]</div>
<p>注意 <span class="math notranslate nohighlight">\(\widehat{e}\)</span> 和 <span class="math notranslate nohighlight">\(e\)</span> 是不同的.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X'\widehat{e}=X'M_{X}e=0_{K}\)</span>.</p></li>
<li><p>若 <span class="math notranslate nohighlight">\(x_{i}\)</span>中有一个为常数, 则 <span class="math notranslate nohighlight">\(\sum_{i=1}^{n}\widehat{e}_{i}=0\)</span>.</p></li>
</ul>
<p>(因为 <span class="math notranslate nohighlight">\(X'\widehat{e}=\left[\begin{array}{cccc}
  1 &amp; 1 &amp; \cdots &amp; 1\\
  \heartsuit &amp; \heartsuit &amp; \cdots &amp; \heartsuit\\
  \cdots &amp; \cdots &amp; \ddots &amp; \vdots\\
  \heartsuit &amp; \heartsuit &amp; \cdots &amp; \heartsuit
  \end{array}\right]\left[\begin{array}{c}
  \widehat{e}_{1}\\
  \widehat{e}_{2}\\
  \vdots\\
  \widehat{e}_{n}
  \end{array}\right]=\left[\begin{array}{c}
  0\\
  0\\
  \vdots\\
  0
  \end{array}\right]\)</span> , 第一行运算说明了
<span class="math notranslate nohighlight">\(\sum_{i=1}^{n}\widehat{e}_{i}=0\)</span>. “<span class="math notranslate nohighlight">\(\heartsuit\)</span>” 代表与我们计算无关的值. )</p>
</div>
</div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">We</span> <span class="n">will</span> <span class="n">produce</span> <span class="n">a</span> <span class="n">graph</span> <span class="n">here</span>
</pre></div>
</div>
<p>我们还可以从几何学的角度来理解最小二乘法.</p>
<p>注意到,
<span class="math notranslate nohighlight">\(\mathcal{X}=\left\{ Xb:b\in\mathbb{R}^{K}\right\}\)</span> 是一个由 <span class="math notranslate nohighlight">\(X=\left[X_{\cdot1},\ldots,X_{\cdot K}\right]\)</span> 中的 <span class="math notranslate nohighlight">\(K\)</span> 列生成的线性空间; 如果这些列是线性独立的, 那么 <span class="math notranslate nohighlight">\(X\)</span> 就是 <span class="math notranslate nohighlight">\(K\)</span> 维的. 最小二乘法估计量 <span class="math notranslate nohighlight">\(\widehat \beta\)</span> 使得 <span class="math notranslate nohighlight">\(\left\Vert Y-Xb\right\Vert^2\)</span> 最小化, 也就是说使得 <span class="math notranslate nohighlight">\(\left\Vert Y-Xb\right\Vert\)</span> 最小化 (对于 <span class="math notranslate nohighlight">\(a\geq 0\)</span>, <span class="math notranslate nohighlight">\(a^2\)</span> 是一个单调变换, 不影响最小化的结果) . 换言之, <span class="math notranslate nohighlight">\(X\widehat{\beta}\)</span> 是 <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> 中的一个点——与 <span class="math notranslate nohighlight">\(Y\)</span> 最接近的一个点.</p>
<p>等式 <span class="math notranslate nohighlight">\(Y=X\widehat{\beta}+\widehat{e}\)</span> 将 <span class="math notranslate nohighlight">\(Y\)</span> 分解为两个垂直向量, <span class="math notranslate nohighlight">\(X\widehat{\beta}\)</span> 与 <span class="math notranslate nohighlight">\(\widehat{e}\)</span>, 因为<span class="math notranslate nohighlight">\(\left\langle X\widehat{\beta},\widehat{e}\right\rangle =\widehat{\beta}'X'\widehat{e}=0_{K}^{\prime}\)</span>, 其中<span class="math notranslate nohighlight">\(\left\langle \cdot,\cdot\right\rangle\)</span> 是向量的内积运算. 那么, <span class="math notranslate nohighlight">\(X\widehat{\beta}\)</span> 就是 <span class="math notranslate nohighlight">\(Y\)</span> 在 <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> 上的<em>投影 (projection)</em> , <span class="math notranslate nohighlight">\(\widehat{e}\)</span> 则是对应的<em>投影残差 (projection
residuals)</em>. 根据勾股定理, 自然有</p>
<div class="math notranslate nohighlight">
\[
\left\Vert Y\right\Vert ^{2}=\Vert X\widehat{\beta}\Vert^{2}+\left\Vert \widehat{e}\right\Vert ^{2}.
\]</div>
<div class="proof example admonition" id="example1">
<p class="admonition-title"><span class="caption-number">Example 1.1 </span></p>
<div class="example-content section" id="proof-content">
<p>下面是一个简单的数值模拟案例, 我们以此来说明最小二乘法估计量的特性. 给定
<span class="math notranslate nohighlight">\(\left(x_{1i},x_{2i},x_{3i},e_{i}\right)^{\prime}\sim N\left(0_{4},I_{4}\right)\)</span>, 因变量 <span class="math notranslate nohighlight">\(y_{i}\)</span> 的生成式为
$<span class="math notranslate nohighlight">\(
y_{i}=0.5+2\cdot x_{1i}-1\cdot x_{2i}+e_{i}
\)</span>$</p>
<p>在不知道 <span class="math notranslate nohighlight">\(x_{3i}\)</span> 是多余的情况下, 我们将 <span class="math notranslate nohighlight">\(y_{i}\)</span> 对
<span class="math notranslate nohighlight">\(\left(1,x_{1i},x_{2i},x_{3i}\right)\)</span> 进行回归.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">library</span><span class="p">(</span><span class="n">magrittr</span><span class="p">);</span> <span class="nf">set.seed</span><span class="p">(</span><span class="m">2022-6-15</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="m">20</span> <span class="c1"># sample size </span>
<span class="n">K</span> <span class="o">=</span> <span class="m">4</span> <span class="c1"># number of paramters</span>
<span class="n">b0</span> <span class="o">=</span> <span class="nf">as.matrix</span><span class="p">(</span> <span class="nf">c</span><span class="p">(</span><span class="m">0.5</span><span class="p">,</span> <span class="m">2</span><span class="p">,</span> <span class="m">-1</span><span class="p">,</span> <span class="m">0</span><span class="p">)</span> <span class="p">)</span> <span class="c1"># the true coefficient</span>
<span class="n">X</span> <span class="o">=</span> <span class="nf">cbind</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="nf">matrix</span><span class="p">(</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">K</span><span class="m">-1</span><span class="p">)),</span> <span class="n">nrow</span> <span class="o">=</span> <span class="n">n</span> <span class="p">)</span> <span class="p">)</span> <span class="c1"># the regressor matrix </span>
<span class="n">e</span> <span class="o">=</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># the error term</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">%*%</span> <span class="n">b0</span> <span class="o">+</span> <span class="n">e</span> <span class="c1"># generate the dependent variable</span>
<span class="n">bhat</span> <span class="o">=</span> <span class="nf">solve</span><span class="p">(</span><span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X</span><span class="p">,</span> <span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">Y</span> <span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">as.vector</span><span class="p">()</span> <span class="o">%&gt;%</span> <span class="nf">print</span><span class="p">()</span>
</pre></div>
</div>
<p>最后得到的参数估计值与真实值已经很接近. 当然, 由于样本容量较小, 答案不是十分准确.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">ehat</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">%*%</span> <span class="n">bhat</span> 
<span class="nf">as.vector</span><span class="p">(</span> <span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">ehat</span> <span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">print</span><span class="p">()</span>
<span class="n">MX</span> <span class="o">=</span> <span class="nf">diag</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">-</span> <span class="n">X</span> <span class="o">%*%</span> <span class="nf">solve</span><span class="p">(</span> <span class="nf">crossprod</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="p">)</span> <span class="o">%*%</span> <span class="nf">t</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nf">data.frame</span><span class="p">(</span><span class="n">e</span> <span class="o">=</span> <span class="n">e</span><span class="p">,</span> <span class="n">ehat</span> <span class="o">=</span> <span class="n">ehat</span><span class="p">,</span> <span class="n">MXY</span> <span class="o">=</span> <span class="n">MX</span><span class="o">%*%</span><span class="n">Y</span><span class="p">,</span> <span class="n">MXe</span> <span class="o">=</span> <span class="n">MX</span><span class="o">%*%</span><span class="n">e</span> <span class="p">)</span> <span class="o">%&gt;%</span> <span class="nf">head</span><span class="p">()</span>
<span class="nf">cat</span><span class="p">(</span><span class="s">&quot;The mean of the residual is &quot;</span><span class="p">,</span> <span class="nf">mean</span><span class="p">(</span><span class="n">ehat</span><span class="p">),</span> <span class="s">&quot;.\n&quot;</span><span class="p">)</span>
<span class="nf">cat</span><span class="p">(</span><span class="s">&quot;The mean of the true error term is&quot;</span><span class="p">,</span> <span class="nf">mean</span><span class="p">(</span><span class="n">e</span><span class="p">),</span> <span class="s">&quot;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div></div>
<div class="section" id="subvector">
<h2><span class="section-number">1.2. </span>子向量 (Subvector)<a class="headerlink" href="#subvector" title="Permalink to this headline">¶</a></h2>
<p>根据 Frish-Waugh-Lovell (FWL) 定理, 最小二乘法估计量的子向量有一个特殊的代数性质. 要得到FWL定理, 我们引入分块矩阵的逆矩阵这一概念. 对于一个实对称的正定矩阵
<span class="math notranslate nohighlight">\(A=\begin{pmatrix}A_{11} &amp; A_{12}\\
A_{12}' &amp; A_{22}
\end{pmatrix},\)</span>
它的逆矩阵可以写作</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^{-1}=\begin{pmatrix}\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} &amp; -\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1}A_{12}A_{22}^{-1}\\
-A_{22}^{-1}A_{12}'\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\right)^{-1} &amp; \left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\right)^{-1}
\end{pmatrix}.
\end{split}\]</div>
<p>将此性质运用至最小二乘法估计量. 记 <span class="math notranslate nohighlight">\(X=\left(\begin{array}{cc}
X_{1} &amp; X_{2}\end{array}\right),\)</span> 那么</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\begin{pmatrix}\widehat{\beta}_{1}\\
\widehat{\beta}_{2}
\end{pmatrix} &amp; =\widehat{\beta}=(X'X)^{-1}X'Y\\
 &amp; =\left(\begin{pmatrix}X_{1}'\\
X_{2}'
\end{pmatrix}\begin{pmatrix}X_{1} &amp; X_{2}\end{pmatrix}\right)^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 &amp; =\begin{pmatrix}X_{1}'X_{1} &amp; X_{1}'X_{2}\\
X_{2}'X_{1} &amp; X_{2}'X_{2}
\end{pmatrix}^{-1}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}\\
 &amp; =\begin{pmatrix}\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1} &amp; -\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}\\
\heartsuit &amp; \heartsuit
\end{pmatrix}\begin{pmatrix}X_{1}'Y\\
X_{2}'Y
\end{pmatrix}.\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\widehat{\beta}\)</span> 的子向量可写作</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{\beta}_{1} &amp; =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'X_{2}\left(X_{2}'X_{2}\right)^{-1}X_{2}'Y\\
 &amp; =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'Y-\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'P_{X_{2}}Y\\
 &amp; =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\right)\\
 &amp; =\left(X_{1}'M_{X_{2}}'X_{1}\right)^{-1}X_{1}'M_{X_{2}}Y.\end{aligned}
\end{split}\]</div>
<p>注意, 我们可以用三步方法得到 <span class="math notranslate nohighlight">\(\widehat{\beta}_{1}\)</span>:</p>
<ol class="simple">
<li><p>将 <span class="math notranslate nohighlight">\(Y\)</span> 对 <span class="math notranslate nohighlight">\(X_{2}\)</span> 做回归, 得到残差 <span class="math notranslate nohighlight">\(\tilde{Y}\)</span>;</p></li>
<li><p>将 <span class="math notranslate nohighlight">\(X_{1}\)</span> 对 <span class="math notranslate nohighlight">\(X_{2}\)</span> 做回归, 得到残差 <span class="math notranslate nohighlight">\(\tilde{X}_{1}\)</span>;</p></li>
<li><p>将 <span class="math notranslate nohighlight">\(\tilde{Y}\)</span> 对 <span class="math notranslate nohighlight">\(\tilde{X}_{1}\)</span> 做回归, 得到最小二乘法估计值 <span class="math notranslate nohighlight">\(\widehat{\beta}_{1}\)</span>.</p></li>
</ol>
<p>同样的方法也适用于总体线性投影 (population linear
projection) , 参考Hansen (2020) [E] Chapter 2.22-23.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X1</span> <span class="o">=</span> <span class="n">X</span><span class="p">[,</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">];</span><span class="n">X2</span> <span class="o">=</span> <span class="n">X</span><span class="p">[,</span><span class="mi">3</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
<span class="n">PX2</span> <span class="o">=</span> <span class="n">X2</span> <span class="o">%*%</span> <span class="n">solve</span><span class="p">(</span> <span class="n">t</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">X2</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">t</span><span class="p">(</span><span class="n">X2</span><span class="p">)</span> 
<span class="n">MX2</span> <span class="o">=</span> <span class="n">diag</span><span class="p">(</span><span class="n">rep</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="p">))</span> <span class="o">-</span> <span class="n">PX2</span>

<span class="n">bhat1</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">solve</span><span class="p">(</span><span class="n">t</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span><span class="o">%*%</span> <span class="n">MX2</span> <span class="o">%*%</span> <span class="n">X1</span><span class="p">,</span> <span class="n">t</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span> <span class="o">%*%</span> <span class="n">MX2</span> <span class="o">%*%</span> <span class="n">Y</span> <span class="p">))</span> <span class="o">%&gt;%</span>
 <span class="k">as</span><span class="o">.</span><span class="n">vector</span><span class="p">()</span> <span class="o">%&gt;%</span> <span class="nb">print</span><span class="p">()</span>

<span class="n">ehat1</span> <span class="o">=</span> <span class="n">MX2</span> <span class="o">%*%</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">MX2</span> <span class="o">%*%</span> <span class="n">X1</span> <span class="o">%*%</span> <span class="n">bhat1</span> 
<span class="n">data</span><span class="o">.</span><span class="n">frame</span><span class="p">(</span><span class="n">ehat</span> <span class="o">=</span> <span class="n">ehat</span><span class="p">,</span> <span class="n">ehat1</span> <span class="o">=</span> <span class="n">ehat1</span><span class="p">)</span> <span class="o">%&gt;%</span> <span class="n">head</span><span class="p">()</span> <span class="o">%&gt;%</span> <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="goodness-of-fit">
<h2><span class="section-number">1.3. </span>适配度检验 (Goodness of Fit)<a class="headerlink" href="#goodness-of-fit" title="Permalink to this headline">¶</a></h2>
<p>考虑回归方程
<span class="math notranslate nohighlight">\(Y=X_{1}\beta_{1}+\beta_{2}+e\)</span>. 代入最小二乘法估计量, 我们得到</p>
<div class="math notranslate nohighlight" id="equation-eqn-2">
<span class="eqno">(1.2)<a class="headerlink" href="#equation-eqn-2" title="Permalink to this equation">¶</a></span>\[
Y=\widehat{Y}+\widehat{e}=\left(X_{1}\widehat{\beta}_{1}+\widehat{\beta}_{2}\right)+\widehat{e}
\]</div>
<p>应用FWL定理, 令 <span class="math notranslate nohighlight">\(X_{2}=\iota\)</span>, 其中希腊字母 <span class="math notranslate nohighlight">\(\iota\)</span> (iota)是一个所有位置都为1的 <span class="math notranslate nohighlight">\(n\times1\)</span> 向量. 那么<span class="math notranslate nohighlight">\(M_{X_{2}}=M_{\iota}=I_{n}-\frac{1}{n}\iota\iota'\)</span>. 其中, <span class="math notranslate nohighlight">\(M_{\iota}z=z-\bar{z}\)</span>. 也就是说, 我们在原本的向量 <span class="math notranslate nohighlight">\(z\)</span> 中减去其均值 <span class="math notranslate nohighlight">\(\bar{z}=\frac{1}{n}\sum_{i=1}^{n}z_{i}\)</span>.</p>
<p>那么, 上述的三步方法变为</p>
<ol class="simple">
<li><p>将 <span class="math notranslate nohighlight">\(Y\)</span> 对 <span class="math notranslate nohighlight">\(\iota\)</span> 做回归, 得到残差 <span class="math notranslate nohighlight">\(M_{\iota}Y\)</span>;</p></li>
<li><p>将 <span class="math notranslate nohighlight">\(X_{1}\)</span> 对 <span class="math notranslate nohighlight">\(\iota\)</span> 做回归, 得到残差 <span class="math notranslate nohighlight">\(M_{\iota}X_{1}\)</span>;</p></li>
<li><p>将 <span class="math notranslate nohighlight">\(M_{\iota}Y\)</span> 对 <span class="math notranslate nohighlight">\(M_{\iota}X_{1}\)</span> 做回归, 得到最小二乘法估计值 <span class="math notranslate nohighlight">\(\widehat{\beta}_{1}\)</span> ——与 <span class="math notranslate nohighlight">\((3.2)\)</span> 中得到的结果完全一致.</p></li>
</ol>
<p>我们将最后一步进行分解</p>
<div class="math notranslate nohighlight" id="equation-eqn-3">
<span class="eqno">(1.3)<a class="headerlink" href="#equation-eqn-3" title="Permalink to this equation">¶</a></span>\[
M_{\iota}Y=M_{\iota}X_{1}\widehat{\beta}_{1}+\tilde{e},
\]</div>
<p>应用勾股定理得到</p>
<div class="math notranslate nohighlight">
\[
\left\Vert M_{\iota}Y\right\Vert ^{2}=\Vert M_{\iota}X_{1}\widehat{\beta}_{1}\Vert^{2}+\left\Vert \widehat{e}\right\Vert ^{2}.
\]</div>
<div class="exercise admonition" id="exercise31">

<p class="admonition-title"><span class="caption-number">Exercise 1.1 </span></p>
<div class="section" id="exercise-content">
<p>试说明: <a class="reference internal" href="#equation-eqn-2">(1.2)</a> 式中的 <span class="math notranslate nohighlight">\(\widehat{e}\)</span> 与 <a class="reference internal" href="#equation-eqn-3">(1.3)</a> 式中的 <span class="math notranslate nohighlight">\(\tilde{e}\)</span> 相等.</p>
</div>
</div>
<p>在线性回归中, <em>决定系数</em> (<span class="math notranslate nohighlight">\(R^2\)</span>) 是一个衡量适配度的指标. 样本内决定系数(in-sample <span class="math notranslate nohighlight">\(R^2\)</span>)可写作</p>
<div class="math notranslate nohighlight">
\[
R^{2}=\frac{\Vert M_{\iota}X_{1}\widehat{\beta}_{1}\Vert^{2}}{\left\Vert M_{\iota}Y\right\Vert ^{2}}=1-\frac{\left\Vert \tilde{e}\right\Vert ^{2}}{\left\Vert M_{\iota}Y\right\Vert ^{2}}.
\]</div>
<p>仅当回归因子包含常数时, <span class="math notranslate nohighlight">\(R^2\)</span> 才有定义.</p>
<div class="exercise admonition" id="exercise32">

<p class="admonition-title"><span class="caption-number">Exercise 1.2 </span></p>
<div class="section" id="exercise-content">
<p>在 <a class="reference internal" href="#equation-eqn-2">(1.2)</a> 式的分解下, 证明</p>
<div class="math notranslate nohighlight">
\[
R^{2}=\frac{\widehat{Y}'M_{\iota}\widehat{Y}}{Y'M_{\iota}Y}=\frac{\sum_{i=1}^{n}\left(\widehat{y_{i}}-\overline{y}\right)^{2}}{\sum_{i=1}^{n}\left(y_{i}-\overline{y}\right)^{2}}.
\]</div>
<p>注意, 上述 <span class="math notranslate nohighlight">\(R^{2}\)</span> 即为<span class="math notranslate nohighlight">\(\widehat{Y}\)</span>的样本方差与 <span class="math notranslate nohighlight">\(Y\)</span> 的样本方差之比值.</p>
</div>
</div>
<p>决定系数 <span class="math notranslate nohighlight">\(R^2\)</span> 的大小在不同的实际问题下差别很大. 在具有滞后效应的宏观模型中, 遇到 <span class="math notranslate nohighlight">\(R^2\)</span> 大于90%的情况并不罕见. 然而, 在横向研究 (cross sectional regressions) 中, <span class="math notranslate nohighlight">\(R^2\)</span> 的值经常是低于20%的.</p>
<div class="exercise admonition" id="exercise33">

<p class="admonition-title"><span class="caption-number">Exercise 1.3 </span></p>
<div class="section" id="exercise-content">
<p>考虑一个较“短”的回归 “将 <span class="math notranslate nohighlight">\(y_{i}\)</span> 对 <span class="math notranslate nohighlight">\(x_{1i}\)</span> 做回归” 与一个较“长”的回归
“将 <span class="math notranslate nohighlight">\(y_{i}\)</span> 对 <span class="math notranslate nohighlight">\(\left(x_{1i},x_{2i}\right)\)</span> 做回归”: 给定相同的数据集 <span class="math notranslate nohighlight">\(\left(Y,X_{1},X_{2}\right)\)</span>, 试说明“短”回归的 <span class="math notranslate nohighlight">\(R^2\)</span> 不比“长”回归的 <span class="math notranslate nohighlight">\(R^2\)</span> 大. (也就是说, 通过增加更多的回归因子, 我们总能使得 <span class="math notranslate nohighlight">\(R^2\)</span> 增大或者不变. )</p>
</div>
</div>
<p>在传统意义上, 回归因子的数量 <span class="math notranslate nohighlight">\(K\)</span> 总是远小于样本量 <span class="math notranslate nohighlight">\(n\)</span> . 然而, 在大数据时代, (潜在) 回归因子的数量 <span class="math notranslate nohighlight">\(K\)</span> 有可能大于样本量 <span class="math notranslate nohighlight">\(n\)</span>.</p>
<div class="exercise admonition" id="exercise34">

<p class="admonition-title"><span class="caption-number">Exercise 1.4 </span></p>
<div class="section" id="exercise-content">
<p>证明: 当 <span class="math notranslate nohighlight">\(K\geq n\)</span> 时, <span class="math notranslate nohighlight">\(R^{2}=1\)</span>. 注意, 若 <span class="math notranslate nohighlight">\(K&gt;n\)</span>, 则矩阵 <span class="math notranslate nohighlight">\(X'X\)</span> 是缺秩 (rank deficient) 的. 在这种情况下, 我们可以将最小二乘法的定义扩展, <span class="math notranslate nohighlight">\(\hat \beta\)</span> 仍然是使得 <span class="math notranslate nohighlight">\(\left\Vert Y-Xb\right\Vert ^{2}\)</span> 最小化的参数值, 但这个值不是唯一的.</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="m">5</span><span class="p">;</span> <span class="n">K</span> <span class="o">=</span> <span class="m">6</span><span class="p">;</span> 
<span class="n">Y</span> <span class="o">=</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">K</span><span class="p">),</span> <span class="n">n</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span> <span class="nf">lm</span><span class="p">(</span><span class="n">Y</span><span class="o">~</span><span class="n">X</span><span class="p">)</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>对于一个新的数据集 <span class="math notranslate nohighlight">\(\left(Y^{\mathrm{new}},X^{\mathrm{new}}\right)\)</span>, 样本外决定系数(OOS <span class="math notranslate nohighlight">\(R^2\)</span>) 可写作</p>
<div class="math notranslate nohighlight">
\[
OOS\ R^{2}=\frac{\widehat{\beta}^{\prime}X^{\mathrm{new}\prime}M_{\iota}X^{\mathrm{new}}\widehat{\beta}}{Y^{\mathrm{new}\prime}M_{\iota}Y^{\mathrm{new}}}.
\]</div>
<p>从原始数据集中得到参数估计值以后, <span class="math notranslate nohighlight">\(OOS\ R^{2}\)</span> 衡量这一参数估计值在新数据集上的适配度. 在金融市场的短期预测模型中, 如果某策略能够系统性地获得2%的 <span class="math notranslate nohighlight">\(OOS\ R^{2}\)</span>, 那么这就是一个很好的赚钱策略.</p>
</div>
<div class="section" id="id2">
<h2><span class="section-number">1.4. </span>总结<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>以上有关线性代数的性质, 在有限样本中是成立的, 无论数据是固定数字或是随机变量. 然而高斯马尔科夫定理 (Gauss Markov theorem) 只在两个关键假设下成立: 线性的条件期望方程 (linear CEF) 和同方差性 (homoscedasticity) .</p>
<div class="admonition- admonition">
<p class="admonition-title">历史趣闻</p>
<p>Carl Friedrich Gauss (1777–1855) 宣称他在1795年就发明了最小二乘法, 那时他用三个数据点预测了1801年矮行星谷神星的位置. 虽然 Gauss 在1809年才发表了相关文章, 而 Adrien-Marie Legendre (1752–1833) 在1805年就公开提出了此方法, 但今天人们仍倾向于将最小二乘法的发明归功于 Gauss. 因为像 Gauss 这样的数学巨人没有必要通过撒谎来窃取 Legendre 的成果.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">介绍</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="lecture5-CN.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2. </span>基本渐近理论</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Zhentao Shi<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>