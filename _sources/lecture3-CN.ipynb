{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最小二乘法：线性代数观点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} 《九章算术》第八章：方程\n",
    "今有卖牛二、羊五, 以买十三豕, 有馀钱一千. 卖牛三、豕三, 以买九羊, 钱适足. 卖羊六、豕八, 以买五牛, 钱不足六百. 问牛、羊、豕价各几何?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left[\\begin{array}{r} \n",
    "2 & 5 & -13\\\\\n",
    "3 & -9 & 3\\\\\n",
    "-5 & 6 & 8\n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c}\n",
    "牛\\\\\n",
    "羊\\\\\n",
    "猪\n",
    "\\end{array} \\right]\n",
    "=\n",
    "\\left[\\begin{array}{r}\n",
    "1000\\\\\n",
    "0\\\\\n",
    "-600\n",
    "\\end{array} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "A = matrix(c(2, 3, -5, 5, -9, 6, -13, 3, 8), nrow = 3)\n",
    "b = c(1000, 0, -600)\n",
    "solve(A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "最小二乘法 (OLS) 是计量经济学中最基本的估计模型, 它简单透明, 易于理解. 了解最小二乘法, 有助于我们研究更复杂的线性估计方法. 此外, 许多非线性估计量在真实值附近与线性估计量的行为是类似的. 在本讲义中, 我们将从线性代数的运算讲起, 学习一系列的知识. \n",
    "\n",
    "\n",
    "套用Leopold Kronecker的名言 \"上帝创造了整数, 其他都是人的作品\", 我想说 \"高斯创造了最小二乘法, 其他都是应用研究者的作品\". 在科学界, 最小二乘法的受欢迎程度远远超出了人们的想象. 但要注意的是, 最小二乘法是一种纯统计学的方法, 或者说是一种监督式机器学习的方法, 因此它揭示的是相关关系, 而非因果关系. 相反, 经济理论假设因果关系的存在, 然后我们收集数据来检验理论或量化效果. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数学标记: $y_{i}$ 是标量. $x_{i}=\\left(x_{i1},\\ldots,x_{iK}\\right)'$ 是 $K\\times1$ 的向量. $Y=\\left(y_{1},\\ldots,y_{n}\\right)'$ 是 $ n\\times1$ 的向量. \n",
    "\n",
    "$$ \n",
    "X=\\left[\\begin{array}{c}\n",
    "x_{1}'\\\\\n",
    "x_{2}'\\\\\n",
    "\\vdots\\\\\n",
    "x_{n}'\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\cdots & x_{1K}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2K}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{22} & \\cdots & x_{nK}\n",
    "\\end{array}\\right]\n",
    "$$ \n",
    "是 $n\\times K$ 的矩阵. $I_{n}$ 是 $n\\times n$ 的单位矩阵. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 估计量 (Estimator) \n",
    "\n",
    "基于我们已经在前一讲中学习过的线性投影模型\n",
    "\n",
    "$$\n",
    "\\begin{aligned}y & =x'\\beta+e\\end{aligned}, \n",
    "$$\n",
    "\n",
    "投影系数 $\\beta$ 可以写作\n",
    "\n",
    "$$\n",
    "\\beta=\\left(E\\left[xx'\\right]\\right)^{-1}E\\left[xy\\right]. \n",
    "$$(eqn:1)\n",
    "\n",
    "我们从 $\\left(y,x\\right)$ 的联合分布中取出一个观测值, 记作 $\\left(y_{i},x_{i}\\right)$. 重复此过程 $n$ 次得到 $n$ 个观测值, 即 $i=1,\\ldots,n$, 那么我们随即就得到了一个样本 $\\left(y_{i},x_{i}\\right)_{i=1}^{n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:remark}\n",
    ":label: remark31\n",
    "样本 $\\left(y_{i},x_{i}\\right)$ 到底是随机的呢？还是固定的呢？\n",
    "\n",
    "——在我们观测之前, 样本是随机变量, 而随机变量的值是不确定的. 当我们谈起样本的统计学性质时, 我们必须将其视之为随机变量, 因为只有随机变量才有统计学性质, 固定值的统计学性质是无意义的. 而在我们观测之后, 样本的值就确定下来了, 成为固定值, 不能再更改. \n",
    "\n",
    "```\n",
    "\n",
    "```{prf:remark}\n",
    ":label: remark32\n",
    "在实际操作中, 我们手中只有一些给定的数据 (当然, 现在的大数据也可以将文本, 照片声音和图像处理成为数据, 这些数据在计算机当中用0和1来表示) . 我们把这些数据扔给计算机, 让计算机给出一个结果. 在统计学意义上, 我们认为这些数字是从一个概率分布上得出的**思想实验**结果. 思想实验是一个学术用语, 说白了, 它就是一个故事. 在公理体系统治的概率论当中, 这个故事在数学上是自洽的. 但是要知道，数学本身是一个套套逻辑 (tautology) , 而不是科学. \n",
    "\n",
    "而概率模型的科学价值恰恰在于, 它到底在多大程度上能够逼近事实的真相？以及, 它是不是能够帮我们预测一些真相？在这门课中, 我们假设数据来自于某种机制, 我们把这种机制当成真相. 比如, 在线性回归当中, $\\left(y,x\\right)$ 的联合分布就是我们头脑中的真相. 而我们想要研究的线性投影系数 $\\beta$ 即为此真相的某个侧面表现形式. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本均值 (sample mean) 是总体均值 (population mean) 的天然估计量. 将式 {eq}`eqn:1` 中总体均值 $E\\left[\\cdot\\right]$ 替换为样本均值 $\\frac{1}{n}\\sum_{i=1}^{n}\\cdot$, 那么相应的, 最小二乘法系数的估计值就可以写作\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\beta} & =\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}x_{i}'\\right)^{-1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}\\\\\n",
    " & =\\left(\\frac{X'X}{n}\\right)^{-1}\\frac{X'y}{n}=\\left(X'X\\right)^{-1}X'y.\\end{aligned}\n",
    "$$\n",
    "\n",
    "上式假设 $X'X$ 是可逆的. 这是一种理解最小二乘法的观点. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以从最小化残差平方和 $\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'b\\right)^{2}$ 的角度来得到最小二乘法估计量, 这等价于最小化\n",
    "\n",
    "$$\n",
    "Q\\left(b\\right)=\\frac{1}{2n}\\sum_{i=1}^{n}\\left(y_{i}-x_{i}'b\\right)^{2}=\\frac{1}{2n}\\left(Y-Xb\\right)'\\left(Y-Xb\\right)=\\frac{1}{2n}\\left\\Vert Y-Xb\\right\\Vert ^{2}.\n",
    "$$\n",
    "\n",
    "其中, 系数 $\\frac{1}{2n}$ 与 $b$ 无关, 不会影响该最小化问题的解. $\\left\\Vert \\cdot\\right\\Vert$ 是向量的欧几里得范数(Euclidean norm). \n",
    "\n",
    "$Q\\left(b\\right)$ 最小化的一阶条件是\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial b}Q\\left(b\\right)=\\left[\\begin{array}{c}\n",
    "\\partial Q\\left(b\\right)/\\partial b_{1}\\\\\n",
    "\\partial Q\\left(b\\right)/\\partial b_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\partial Q\\left(b\\right)/\\partial b_{K}\n",
    "\\end{array}\\right]=-\\frac{1}{n}X'\\left(Y-Xb\\right)=0.\n",
    "$$\n",
    "\n",
    "因此, 最小化 $Q\\left(b\\right)$ 的必要条件让我们得到了相同的估计值 $b=\\widehat{\\beta}=\\left(X'X\\right)^{-1}X'y$. \n",
    "\n",
    "另外, 二阶条件\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^{2}}{\\partial b\\partial b'}Q\\left(b\\right)=\\left[\\begin{array}{cccc}\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}^{2}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{2}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{1}}Q\\left(b\\right)\\\\\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{2}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}^{2}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}\\partial b_{2}}Q\\left(b\\right)\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial^{2}}{\\partial b_{1}\\partial b_{K}}Q\\left(b\\right) & \\frac{\\partial^{2}}{\\partial b_{2}\\partial b_{K}}Q\\left(b\\right) & \\cdots & \\frac{\\partial^{2}}{\\partial b_{K}^{2}}Q\\left(b\\right)\n",
    "\\end{array}\\right]=\\frac{1}{n}X'X\n",
    "$$\n",
    "\n",
    "表明 $Q\\left(b\\right)$ 是关于 $b$ 的凸函数, 因为 $X'X/n$ 是半正定矩阵. (如果 $X'X/n$ 是正定矩阵, 那么 $Q\\left(b\\right)$ 是关于 $b$ 的严格凸函数. ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:remark}\n",
    ":label: remark3\n",
    "在最小二乘法的推导中, 我们假定 $X$ 中的 $K$ 列是*线性独立的*, 也就是说不存在 $K\\times1$ 的向量\n",
    "$b\\ (b\\neq0_{K})$ 使得 $Xb=0_{n}$. 同时, 这也意味着 $n\\geq K$ , 并且 $X'X/n$ 是可逆矩阵. \n",
    "\n",
    "然而线性独立的假设并不永远正确, 如果某些回归因子满足*完全共线性 (perfectly collinear)* , 就违反了线性独立的假设. \n",
    "\n",
    "例如, 使用虚拟变量来表示分类变量并将所有这些类别放入回归模型中时, 通常计量经济学软件会自动检测并指出完全共线性问题. 然而, 难以察觉的是*不完全共线性 (nearly collinear)*, 即 $X'X/n$ 的最小特征值接近于0, 而不等于0. 我们将在渐进理论一章中讨论不完全共线性的后果. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{prf:property}\n",
    ":label: property1\n",
    "\n",
    "下面列举一些与最小二乘法估计量有关的定义与性质. \n",
    "\n",
    "-  拟合值 (Fitted value): $\\widehat{Y}=X\\widehat{\\beta}$.\n",
    "\n",
    "-  投影矩阵 (Projection matrix): $P_{X}=X\\left(X'X\\right)^{-1}X$; 残差生成矩阵 (Residual maker\n",
    "  matrix) : $M_{X}=I_{n}-P_{X}$.\n",
    "\n",
    "-  $P_{X}X=X$; $X'P_{X}=X'$.\n",
    "\n",
    "-  $M_{X}X=0_{n\\times K}$; $X'M_{X}=0_{K\\times n}$.\n",
    "\n",
    "-  $P_{X}M_{X}=M_{X}P_{X}=0_{n\\times n}$.\n",
    "\n",
    "-  $P_{X}$ 与 $M_{X}$ 都是*幂等矩阵*. \n",
    "  *  如果 $AA=A$, 那么 $A$ 被称作*幂等矩阵* (*idempotent* matrix). 幂等矩阵的特征值只能是1或者0. \n",
    "\n",
    "-  $\\mathrm{rank}\\left(P_{X}\\right)=K$, \n",
    "  $\\mathrm{rank}\\left(M_{X}\\right)=n-K$.\n",
    "\n",
    "- 残差: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{e}&=Y-\\widehat{Y}=Y-X\\widehat{\\beta}=Y-X(X'X)^{-1}X'Y=(I_{n}-P_{X})Y=M_{X}Y\\\\\n",
    "&=M_{X}\\left(X\\beta+e\\right)=M_{X}e.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "  注意 $\\widehat{e}$ 和 $e$ 是不同的. \n",
    "\n",
    "-  $X'\\widehat{e}=X'M_{X}e=0_{K}$.\n",
    "\n",
    "-  若 $x_{i}$中有一个为常数, 则 $\\sum_{i=1}^{n}\\widehat{e}_{i}=0$. \n",
    "\n",
    "  (因为 $X'\\widehat{e}=\\left[\\begin{array}{cccc}\n",
    "  1 & 1 & \\cdots & 1\\\\\n",
    "  \\heartsuit & \\heartsuit & \\cdots & \\heartsuit\\\\\n",
    "  \\cdots & \\cdots & \\ddots & \\vdots\\\\\n",
    "  \\heartsuit & \\heartsuit & \\cdots & \\heartsuit\n",
    "  \\end{array}\\right]\\left[\\begin{array}{c}\n",
    "  \\widehat{e}_{1}\\\\\n",
    "  \\widehat{e}_{2}\\\\\n",
    "  \\vdots\\\\\n",
    "  \\widehat{e}_{n}\n",
    "  \\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "  0\\\\\n",
    "  0\\\\\n",
    "  \\vdots\\\\\n",
    "  0\n",
    "  \\end{array}\\right]$ , 第一行运算说明了\n",
    "  $\\sum_{i=1}^{n}\\widehat{e}_{i}=0$. \"$\\heartsuit$\" 代表与我们计算无关的值. )\n",
    "  \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "We will produce a graph here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以从几何学的角度来理解最小二乘法. \n",
    "\n",
    "注意到, \n",
    "$\\mathcal{X}=\\left\\{ Xb:b\\in\\mathbb{R}^{K}\\right\\}$ 是一个由 $X=\\left[X_{\\cdot1},\\ldots,X_{\\cdot K}\\right]$ 中的 $K$ 列生成的线性空间; 如果这些列是线性独立的, 那么 $X$ 就是 $K$ 维的. 最小二乘法估计量 $\\widehat \\beta$ 使得 $\\left\\Vert Y-Xb\\right\\Vert^2$ 最小化, 也就是说使得 $\\left\\Vert Y-Xb\\right\\Vert$ 最小化 (对于 $a\\geq 0$, $a^2$ 是一个单调变换, 不影响最小化的结果) . 换言之, $X\\widehat{\\beta}$ 是 $\\mathcal{X}$ 中的一个点——与 $Y$ 最接近的一个点. \n",
    "\n",
    "\n",
    "等式 $Y=X\\widehat{\\beta}+\\widehat{e}$ 将 $Y$ 分解为两个垂直向量, $X\\widehat{\\beta}$ 与 $\\widehat{e}$, 因为$\\left\\langle X\\widehat{\\beta},\\widehat{e}\\right\\rangle =\\widehat{\\beta}'X'\\widehat{e}=0_{K}^{\\prime}$, 其中$\\left\\langle \\cdot,\\cdot\\right\\rangle$ 是向量的内积运算. 那么, $X\\widehat{\\beta}$ 就是 $Y$ 在 $\\mathcal{X}$ 上的*投影 (projection)* , $\\widehat{e}$ 则是对应的*投影残差 (projection\n",
    "residuals)*. 根据勾股定理, 自然有\n",
    "\n",
    "$$\n",
    "\\left\\Vert Y\\right\\Vert ^{2}=\\Vert X\\widehat{\\beta}\\Vert^{2}+\\left\\Vert \\widehat{e}\\right\\Vert ^{2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{prf:example}\n",
    ":label: example1\n",
    "\n",
    "下面是一个简单的数值模拟案例, 我们以此来说明最小二乘法估计量的特性. 给定\n",
    "$\\left(x_{1i},x_{2i},x_{3i},e_{i}\\right)^{\\prime}\\sim N\\left(0_{4},I_{4}\\right)$, 因变量 $y_{i}$ 的生成式为\n",
    "$$\n",
    "y_{i}=0.5+2\\cdot x_{1i}-1\\cdot x_{2i}+e_{i}\n",
    "$$\n",
    "\n",
    "在不知道 $x_{3i}$ 是多余的情况下, 我们将 $y_{i}$ 对\n",
    "$\\left(1,x_{1i},x_{2i},x_{3i}\\right)$ 进行回归. \n",
    "\n",
    "```r\n",
    "library(magrittr); set.seed(2022-6-15)\n",
    "n = 20 # sample size \n",
    "K = 4 # number of paramters\n",
    "b0 = as.matrix( c(0.5, 2, -1, 0) ) # the true coefficient\n",
    "X = cbind(1, matrix( rnorm(n * (K-1)), nrow = n ) ) # the regressor matrix \n",
    "e = rnorm(n) # the error term\n",
    "Y = X %*% b0 + e # generate the dependent variable\n",
    "bhat = solve(t(X) %*% X, t(X) %*% Y ) %>% as.vector() %>% print()\n",
    "```\n",
    "\n",
    "最后得到的参数估计值与真实值已经很接近. 当然, 由于样本容量较小, 答案不是十分准确. \n",
    "\n",
    "```r\n",
    "ehat = Y - X %*% bhat \n",
    "as.vector( t(X) %*% ehat ) %>% print()\n",
    "MX = diag(n) - X %*% solve( crossprod(X) ) %*% t(X)\n",
    "data.frame(e = e, ehat = ehat, MXY = MX%*%Y, MXe = MX%*%e ) %>% head()\n",
    "cat(\"The mean of the residual is \", mean(ehat), \".\\n\")\n",
    "cat(\"The mean of the true error term is\", mean(e), \".\")\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子向量 (Subvector) \n",
    "\n",
    "根据 Frish-Waugh-Lovell (FWL) 定理, 最小二乘法估计量的子向量有一个特殊的代数性质. 要得到FWL定理, 我们引入分块矩阵的逆矩阵这一概念. 对于一个实对称的正定矩阵\n",
    "$A=\\begin{pmatrix}A_{11} & A_{12}\\\\\n",
    "A_{12}' & A_{22}\n",
    "\\end{pmatrix},$\n",
    "它的逆矩阵可以写作\n",
    "\n",
    "$$\n",
    "A^{-1}=\\begin{pmatrix}\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & -\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1}A_{12}A_{22}^{-1}\\\\\n",
    "-A_{22}^{-1}A_{12}'\\left(A_{11}-A_{12}A_{22}^{-1}A_{12}'\\right)^{-1} & \\left(A_{22}-A_{12}'A_{11}^{-1}A_{12}\\right)^{-1}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "将此性质运用至最小二乘法估计量. 记 $X=\\left(\\begin{array}{cc}\n",
    "X_{1} & X_{2}\\end{array}\\right),$ 那么\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\begin{pmatrix}\\widehat{\\beta}_{1}\\\\\n",
    "\\widehat{\\beta}_{2}\n",
    "\\end{pmatrix} & =\\widehat{\\beta}=(X'X)^{-1}X'Y\\\\\n",
    " & =\\left(\\begin{pmatrix}X_{1}'\\\\\n",
    "X_{2}'\n",
    "\\end{pmatrix}\\begin{pmatrix}X_{1} & X_{2}\\end{pmatrix}\\right)^{-1}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}\\\\\n",
    " & =\\begin{pmatrix}X_{1}'X_{1} & X_{1}'X_{2}\\\\\n",
    "X_{2}'X_{1} & X_{2}'X_{2}\n",
    "\\end{pmatrix}^{-1}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}\\\\\n",
    " & =\\begin{pmatrix}\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1} & -\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}\\\\\n",
    "\\heartsuit & \\heartsuit\n",
    "\\end{pmatrix}\\begin{pmatrix}X_{1}'Y\\\\\n",
    "X_{2}'Y\n",
    "\\end{pmatrix}.\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\widehat{\\beta}$ 的子向量可写作 \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\beta}_{1} & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'Y-\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'X_{2}\\left(X_{2}'X_{2}\\right)^{-1}X_{2}'Y\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'Y-\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'P_{X_{2}}Y\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}\\left(X_{1}'Y-X_{1}'P_{X_{2}}Y\\right)\\\\\n",
    " & =\\left(X_{1}'M_{X_{2}}'X_{1}\\right)^{-1}X_{1}'M_{X_{2}}Y.\\end{aligned}\n",
    "$$\n",
    "\n",
    "注意, 我们可以用三步方法得到 $\\widehat{\\beta}_{1}$: \n",
    "\n",
    "1. 将 $Y$ 对 $X_{2}$ 做回归, 得到残差 $\\tilde{Y}$;\n",
    "\n",
    "2. 将 $X_{1}$ 对 $X_{2}$ 做回归, 得到残差 $\\tilde{X}_{1}$;\n",
    "\n",
    "3. 将 $\\tilde{Y}$ 对 $\\tilde{X}_{1}$ 做回归, 得到最小二乘法估计值 $\\widehat{\\beta}_{1}$.\n",
    "\n",
    "同样的方法也适用于总体线性投影 (population linear\n",
    "projection) , 参考Hansen (2020) \\[E\\] Chapter 2.22-23. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X1 = X[,1:2];X2 = X[,3:4]\n",
    "PX2 = X2 %*% solve( t(X2) %*% X2) %*% t(X2) \n",
    "MX2 = diag(rep(1,n)) - PX2\n",
    "\n",
    "bhat1 <- (solve(t(X1)%*% MX2 %*% X1, t(X1) %*% MX2 %*% Y )) %>%\n",
    " as.vector() %>% print()\n",
    "\n",
    "ehat1 = MX2 %*% Y - MX2 %*% X1 %*% bhat1 \n",
    "data.frame(ehat = ehat, ehat1 = ehat1) %>% head() %>% print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 适配度检验 (Goodness of Fit) \n",
    "\n",
    "考虑回归方程\n",
    "$Y=X_{1}\\beta_{1}+\\beta_{2}+e$. 代入最小二乘法估计量, 我们得到\n",
    "\n",
    "$$\n",
    "Y=\\widehat{Y}+\\widehat{e}=\\left(X_{1}\\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}\\right)+\\widehat{e}\n",
    "$$(eqn:2)\n",
    "\n",
    "应用FWL定理, 令 $X_{2}=\\iota$, 其中希腊字母 $\\iota$ (iota)是一个所有位置都为1的 $n\\times1$ 向量. 那么$M_{X_{2}}=M_{\\iota}=I_{n}-\\frac{1}{n}\\iota\\iota'$. 其中, $M_{\\iota}z=z-\\bar{z}$. 也就是说, 我们在原本的向量 $z$ 中减去其均值 $\\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n}z_{i}$. \n",
    "\n",
    "那么, 上述的三步方法变为\n",
    "\n",
    "1. 将 $Y$ 对 $\\iota$ 做回归, 得到残差 $M_{\\iota}Y$;\n",
    "\n",
    "2. 将 $X_{1}$ 对 $\\iota$ 做回归, 得到残差 $M_{\\iota}X_{1}$;\n",
    "\n",
    "3. 将 $M_{\\iota}Y$ 对 $M_{\\iota}X_{1}$ 做回归, 得到最小二乘法估计值 $\\widehat{\\beta}_{1}$ ——与 $(3.2)$ 中得到的结果完全一致. \n",
    "\n",
    "\n",
    "我们将最后一步进行分解\n",
    "\n",
    "$$\n",
    "M_{\\iota}Y=M_{\\iota}X_{1}\\widehat{\\beta}_{1}+\\tilde{e},\n",
    "$$(eqn:3)\n",
    "\n",
    "应用勾股定理得到\n",
    "\n",
    "$$\n",
    "\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}=\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}+\\left\\Vert \\widehat{e}\\right\\Vert ^{2}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: exercise31\n",
    "\n",
    "试说明: {eq}`eqn:2` 式中的 $\\widehat{e}$ 与 {eq}`eqn:3` 式中的 $\\tilde{e}$ 相等. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性回归中, *决定系数* ($R^2$) 是一个衡量适配度的指标. 样本内决定系数(in-sample $R^2$)可写作\n",
    "\n",
    "$$\n",
    "R^{2}=\\frac{\\Vert M_{\\iota}X_{1}\\widehat{\\beta}_{1}\\Vert^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}=1-\\frac{\\left\\Vert \\tilde{e}\\right\\Vert ^{2}}{\\left\\Vert M_{\\iota}Y\\right\\Vert ^{2}}.\n",
    "$$\n",
    "\n",
    "仅当回归因子包含常数时, $R^2$ 才有定义. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: exercise32\n",
    "\n",
    "在 {eq}`eqn:2` 式的分解下, 证明\n",
    "\n",
    "$$\n",
    "R^{2}=\\frac{\\widehat{Y}'M_{\\iota}\\widehat{Y}}{Y'M_{\\iota}Y}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{y_{i}}-\\overline{y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}}.\n",
    "$$\n",
    "注意, 上述 $R^{2}$ 即为$\\widehat{Y}$的样本方差与 $Y$ 的样本方差之比值. \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "决定系数 $R^2$ 的大小在不同的实际问题下差别很大. 在具有滞后效应的宏观模型中, 遇到 $R^2$ 大于90%的情况并不罕见. 然而, 在横向研究 (cross sectional regressions) 中, $R^2$ 的值经常是低于20%的. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{exercise}\n",
    ":label: exercise33\n",
    "\n",
    "考虑一个较“短”的回归 \"将 $y_{i}$ 对 $x_{1i}$ 做回归\" 与一个较“长”的回归\n",
    " \"将 $y_{i}$ 对 $\\left(x_{1i},x_{2i}\\right)$ 做回归\": 给定相同的数据集 $\\left(Y,X_{1},X_{2}\\right)$, 试说明“短”回归的 $R^2$ 不比“长”回归的 $R^2$ 大. (也就是说, 通过增加更多的回归因子, 我们总能使得 $R^2$ 增大或者不变. ) \n",
    "```\n",
    "\n",
    "在传统意义上, 回归因子的数量 $K$ 总是远小于样本量 $n$ . 然而, 在大数据时代, (潜在) 回归因子的数量 $K$ 有可能大于样本量 $n$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{exercise}\n",
    ":label: exercise34\n",
    "\n",
    "证明: 当 $K\\geq n$ 时, $R^{2}=1$. 注意, 若 $K>n$, 则矩阵 $X'X$ 是缺秩 (rank deficient) 的. 在这种情况下, 我们可以将最小二乘法的定义扩展, $\\hat \\beta$ 仍然是使得 $\\left\\Vert Y-Xb\\right\\Vert ^{2}$ 最小化的参数值, 但这个值不是唯一的. \n",
    "\n",
    "```r\n",
    "n = 5; K = 6; \n",
    "Y = rnorm(n)\n",
    "X = matrix( rnorm(n*K), n)\n",
    "summary( lm(Y~X) )\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个新的数据集 $\\left(Y^{\\mathrm{new}},X^{\\mathrm{new}}\\right)$, 样本外决定系数(OOS $R^2$) 可写作\n",
    "\n",
    "$$\n",
    "OOS\\ R^{2}=\\frac{\\widehat{\\beta}^{\\prime}X^{\\mathrm{new}\\prime}M_{\\iota}X^{\\mathrm{new}}\\widehat{\\beta}}{Y^{\\mathrm{new}\\prime}M_{\\iota}Y^{\\mathrm{new}}}.\n",
    "$$\n",
    "\n",
    "从原始数据集中得到参数估计值以后, $OOS\\ R^{2}$ 衡量这一参数估计值在新数据集上的适配度. 在金融市场的短期预测模型中, 如果某策略能够系统性地获得2\\%的 $OOS\\ R^{2}$, 那么这就是一个很好的赚钱策略. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 总结\n",
    "\n",
    "以上有关线性代数的性质, 在有限样本中是成立的, 无论数据是固定数字或是随机变量. 然而高斯马尔科夫定理 (Gauss Markov theorem) 只在两个关键假设下成立: 线性的条件期望方程 (linear CEF) 和同方差性 (homoscedasticity) . \n",
    "\n",
    "\n",
    "```{admonition} 历史趣闻\n",
    "Carl Friedrich Gauss (1777--1855) 宣称他在1795年就发明了最小二乘法, 那时他用三个数据点预测了1801年矮行星谷神星的位置. 虽然 Gauss 在1809年才发表了相关文章, 而 Adrien-Marie Legendre (1752--1833) 在1805年就公开提出了此方法, 但今天人们仍倾向于将最小二乘法的发明归功于 Gauss. 因为像 Gauss 这样的数学巨人没有必要通过撒谎来窃取 Legendre 的成果. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
